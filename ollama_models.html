<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>model-name</th>
      <th>description</th>
      <th>tags</th>
      <th>pulls-count</th>
      <th>last-updated</th>
      <th>hf-search_url</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>llama2</td>
      <td>The most popular model for general use.</td>
      <td></td>
      <td>https://huggingface.co/search/full-text?q=llama2&type=model</td>
      <td>222100</td>
      <td>2023-12-30</td>
    </tr>
    <tr>
      <td>mistral</td>
      <td>The 7B model released by Mistral AI, updated to version 0.2.</td>
      <td></td>
      <td>https://huggingface.co/search/full-text?q=mistral&type=model</td>
      <td>135100</td>
      <td>2023-12-30</td>
    </tr>
    <tr>
      <td>llava</td>
      <td>ðŸŒ‹ A novel end-to-end trained large multimodal model that combines a vision encoder and Vicuna for general-purpose visual and language understanding.</td>
      <td></td>
      <td>https://huggingface.co/search/full-text?q=llava&type=model</td>
      <td>11100</td>
      <td>2023-12-30</td>
    </tr>
    <tr>
      <td>mixtral</td>
      <td>A high-quality Mixture of Experts (MoE) model with open weights by Mistral AI.</td>
      <td></td>
      <td>https://huggingface.co/search/full-text?q=mixtral&type=model</td>
      <td>26500</td>
      <td>2023-12-30</td>
    </tr>
    <tr>
      <td>starling-lm</td>
      <td>Starling is a large language model trained by reinforcement learning from AI feedback focused on improving chatbot helpfulness.</td>
      <td></td>
      <td>https://huggingface.co/search/full-text?q=starling-lm&type=model</td>
      <td>5573</td>
      <td>2023-12-02</td>
    </tr>
    <tr>
      <td>neural-chat</td>
      <td>A fine-tuned model based on Mistral with good coverage of domain and language.</td>
      <td></td>
      <td>https://huggingface.co/search/full-text?q=neural-chat&type=model</td>
      <td>6822</td>
      <td>2023-12-23</td>
    </tr>
    <tr>
      <td>codellama</td>
      <td>A large language model that can use text prompts to generate and discuss code.</td>
      <td>code</td>
      <td>https://huggingface.co/search/full-text?q=codellama&type=model</td>
      <td>99000</td>
      <td>2023-11-13</td>
    </tr>
    <tr>
      <td>dolphin-mixtral</td>
      <td>An uncensored, fine-tuned model based on the Mixtral mixture of experts model that excels at coding tasks. Created by Eric Hartford.</td>
      <td>code</td>
      <td>https://huggingface.co/search/full-text?q=dolphin-mixtral&type=model</td>
      <td>84600</td>
      <td>2024-01-02</td>
    </tr>
    <tr>
      <td>mistral-openorca</td>
      <td>Mistral OpenOrca is a 7 billion parameter model, fine-tuned on top of the Mistral 7B model using the OpenOrca dataset.</td>
      <td></td>
      <td>https://huggingface.co/search/full-text?q=mistral-openorca&type=model</td>
      <td>61600</td>
      <td>2023-10-13</td>
    </tr>
    <tr>
      <td>llama2-uncensored</td>
      <td>Uncensored Llama 2 model by George Sung and Jarrad Hope.</td>
      <td></td>
      <td>https://huggingface.co/search/full-text?q=llama2-uncensored&type=model</td>
      <td>48400</td>
      <td>2023-11-13</td>
    </tr>
    <tr>
      <td>orca-mini</td>
      <td>A general-purpose model ranging from 3 billion parameters to 70 billion, suitable for entry-level hardware.</td>
      <td></td>
      <td>https://huggingface.co/search/full-text?q=orca-mini&type=model</td>
      <td>36700</td>
      <td>2023-11-13</td>
    </tr>
    <tr>
      <td>vicuna</td>
      <td>General use chat model based on Llama and Llama 2 with 2K to 16K context sizes.</td>
      <td></td>
      <td>https://huggingface.co/search/full-text?q=vicuna&type=model</td>
      <td>24300</td>
      <td>2023-11-13</td>
    </tr>
    <tr>
      <td>wizard-vicuna-uncensored</td>
      <td>Wizard Vicuna Uncensored is a 7B, 13B, and 30B parameter model based on Llama 2 uncensored by Eric Hartford.</td>
      <td></td>
      <td>https://huggingface.co/search/full-text?q=wizard-vicuna-uncensored&type=model</td>
      <td>19900</td>
      <td>2023-11-13</td>
    </tr>
    <tr>
      <td>deepseek-coder</td>
      <td>DeepSeek Coder is a capable coding model trained on two trillion code and natural language tokens.</td>
      <td>code</td>
      <td>https://huggingface.co/search/full-text?q=deepseek-coder&type=model</td>
      <td>16600</td>
      <td>2023-12-30</td>
    </tr>
    <tr>
      <td>zephyr</td>
      <td>Zephyr beta is a fine-tuned 7B version of mistral that was trained on on a mix of publicly available, synthetic datasets.</td>
      <td></td>
      <td>https://huggingface.co/search/full-text?q=zephyr&type=model</td>
      <td>14800</td>
      <td>2023-12-30</td>
    </tr>
    <tr>
      <td>dolphin-mistral</td>
      <td>The uncensored Dolphin model based on Mistral that excels at coding tasks. Updated to version 2.6.</td>
      <td>code</td>
      <td>https://huggingface.co/search/full-text?q=dolphin-mistral&type=model</td>
      <td>13700</td>
      <td>2024-01-11</td>
    </tr>
    <tr>
      <td>phind-codellama</td>
      <td>Code generation model based on Code Llama.</td>
      <td>code</td>
      <td>https://huggingface.co/search/full-text?q=phind-codellama&type=model</td>
      <td>12900</td>
      <td>2023-12-30</td>
    </tr>
    <tr>
      <td>wizardcoder</td>
      <td>State-of-the-art code generation model</td>
      <td>code</td>
      <td>https://huggingface.co/search/full-text?q=wizardcoder&type=model</td>
      <td>12900</td>
      <td>2024-01-06</td>
    </tr>
    <tr>
      <td>phi</td>
      <td>Phi-2: a 2.7B language model by Microsoft Research that demonstrates outstanding reasoning and language understanding capabilities.</td>
      <td>logic</td>
      <td>https://huggingface.co/search/full-text?q=phi&type=model</td>
      <td>12200</td>
      <td>2024-01-06</td>
    </tr>
    <tr>
      <td>llama2-chinese</td>
      <td>Llama 2 based model fine tuned to improve Chinese dialogue ability.</td>
      <td>chinese</td>
      <td>https://huggingface.co/search/full-text?q=llama2-chinese&type=model</td>
      <td>10100</td>
      <td>2023-11-13</td>
    </tr>
    <tr>
      <td>nous-hermes</td>
      <td>General use models based on Llama and Llama 2 from Nous Research.</td>
      <td></td>
      <td>https://huggingface.co/search/full-text?q=nous-hermes&type=model</td>
      <td>9753</td>
      <td>2023-11-13</td>
    </tr>
    <tr>
      <td>orca2</td>
      <td>Orca 2 is built by Microsoft research, and are a fine-tuned version of Meta's Llama 2 models.  The model is designed to excel particularly in reasoning.</td>
      <td>logic</td>
      <td>https://huggingface.co/search/full-text?q=orca2&type=model</td>
      <td>9558</td>
      <td>2023-11-25</td>
    </tr>
    <tr>
      <td>wizard-math</td>
      <td>Model focused on math and logic problems</td>
      <td>logic / math</td>
      <td>https://huggingface.co/search/full-text?q=wizard-math&type=model</td>
      <td>9553</td>
      <td>2023-12-23</td>
    </tr>
    <tr>
      <td>openhermes</td>
      <td>OpenHermes 2.5 is a 7B model fine-tuned by Teknium on Mistral with fully open datasets.</td>
      <td></td>
      <td>https://huggingface.co/search/full-text?q=openhermes&type=model</td>
      <td>9052</td>
      <td>2023-12-30</td>
    </tr>
    <tr>
      <td>falcon</td>
      <td>A large language model built by the Technology Innovation Institute (TII) for use in summarization, text generation, and chat bots.</td>
      <td></td>
      <td>https://huggingface.co/search/full-text?q=falcon&type=model</td>
      <td>8814</td>
      <td>2023-11-13</td>
    </tr>
    <tr>
      <td>openchat</td>
      <td>A family of open-source models trained on a wide variety of data, surpassing ChatGPT on various benchmarks. Updated to version 3.5-0106.</td>
      <td></td>
      <td>https://huggingface.co/search/full-text?q=openchat&type=model</td>
      <td>7467</td>
      <td>2024-01-11</td>
    </tr>
    <tr>
      <td>codeup</td>
      <td>Great code generation model based on Llama2.</td>
      <td>code</td>
      <td>https://huggingface.co/search/full-text?q=codeup&type=model</td>
      <td>7169</td>
      <td>2023-11-13</td>
    </tr>
    <tr>
      <td>stable-beluga</td>
      <td>Llama 2 based model fine tuned on an Orca-style dataset. Originally called Free Willy.</td>
      <td></td>
      <td>https://huggingface.co/search/full-text?q=stable-beluga&type=model</td>
      <td>6606</td>
      <td>2023-11-13</td>
    </tr>
    <tr>
      <td>everythinglm</td>
      <td>Uncensored Llama2 based model with support for a 16K context window.</td>
      <td></td>
      <td>https://huggingface.co/search/full-text?q=everythinglm&type=model</td>
      <td>6216</td>
      <td>2023-12-30</td>
    </tr>
    <tr>
      <td>medllama2</td>
      <td>Fine-tuned Llama 2 model to answer medical questions based on an open source medical dataset.</td>
      <td>medical</td>
      <td>https://huggingface.co/search/full-text?q=medllama2&type=model</td>
      <td>5926</td>
      <td>2023-11-13</td>
    </tr>
    <tr>
      <td>wizardlm-uncensored</td>
      <td>Uncensored version of Wizard LM model</td>
      <td></td>
      <td>https://huggingface.co/search/full-text?q=wizardlm-uncensored&type=model</td>
      <td>5740</td>
      <td>2023-11-13</td>
    </tr>
    <tr>
      <td>starcoder</td>
      <td>StarCoder is a code generation model trained on 80+ programming languages.</td>
      <td>code</td>
      <td>https://huggingface.co/search/full-text?q=starcoder&type=model</td>
      <td>5542</td>
      <td>2023-11-13</td>
    </tr>
    <tr>
      <td>yi</td>
      <td>A high-performing, bilingual language model.</td>
      <td>chinese</td>
      <td>https://huggingface.co/search/full-text?q=yi&type=model</td>
      <td>4719</td>
      <td>2023-12-30</td>
    </tr>
    <tr>
      <td>bakllava</td>
      <td>BakLLaVA is a multimodal model consisting of the Mistral 7B base model augmented with the LLaVA  architecture.</td>
      <td></td>
      <td>https://huggingface.co/search/full-text?q=bakllava&type=model</td>
      <td>4411</td>
      <td>2023-12-16</td>
    </tr>
    <tr>
      <td>yarn-mistral</td>
      <td>An extension of Mistral to support context windows of 64K or 128K.</td>
      <td></td>
      <td>https://huggingface.co/search/full-text?q=yarn-mistral&type=model</td>
      <td>4062</td>
      <td>2023-12-30</td>
    </tr>
    <tr>
      <td>wizard-vicuna</td>
      <td>Wizard Vicuna is a 13B parameter model based on Llama 2 trained by MelodysDreamj.</td>
      <td></td>
      <td>https://huggingface.co/search/full-text?q=wizard-vicuna&type=model</td>
      <td>3978</td>
      <td>2023-11-13</td>
    </tr>
    <tr>
      <td>tinyllama</td>
      <td>The TinyLlama project is an open endeavor to train a compact 1.1B Llama model on 3 trillion tokens.</td>
      <td></td>
      <td>https://huggingface.co/search/full-text?q=tinyllama&type=model</td>
      <td>3880</td>
      <td>2024-01-01</td>
    </tr>
    <tr>
      <td>solar</td>
      <td>A compact, yet powerful 10.7B large language model designed for single-turn conversation.</td>
      <td></td>
      <td>https://huggingface.co/search/full-text?q=solar&type=model</td>
      <td>3717</td>
      <td>2023-12-23</td>
    </tr>
    <tr>
      <td>dolphin-phi</td>
      <td>2.7B uncensored Dolphin model by Eric Hartford, based on the Phi language model by Microsoft Research.</td>
      <td></td>
      <td>https://huggingface.co/search/full-text?q=dolphin-phi&type=model</td>
      <td>3577</td>
      <td>2023-12-30</td>
    </tr>
    <tr>
      <td>open-orca-platypus2</td>
      <td>Merge of the Open Orca OpenChat model and the Garage-bAInd Platypus 2 model. Designed for chat and code generation.</td>
      <td>code</td>
      <td>https://huggingface.co/search/full-text?q=open-orca-platypus2&type=model</td>
      <td>3343</td>
      <td>2023-11-13</td>
    </tr>
    <tr>
      <td>samantha-mistral</td>
      <td>A companion assistant trained in philosophy, psychology, and personal relationships. Based on Mistral.</td>
      <td></td>
      <td>https://huggingface.co/search/full-text?q=samantha-mistral&type=model</td>
      <td>3308</td>
      <td>2023-10-13</td>
    </tr>
    <tr>
      <td>sqlcoder</td>
      <td>SQLCoder is a code completion model fined-tuned on StarCoder for SQL generation tasks</td>
      <td>code</td>
      <td>https://huggingface.co/search/full-text?q=sqlcoder&type=model</td>
      <td>3272</td>
      <td>2023-11-13</td>
    </tr>
    <tr>
      <td>meditron</td>
      <td>Open-source medical large language model adapted from Llama 2 to the medical domain.</td>
      <td>medical</td>
      <td>https://huggingface.co/search/full-text?q=meditron&type=model</td>
      <td>3030</td>
      <td>2023-12-09</td>
    </tr>
    <tr>
      <td>stablelm-zephyr</td>
      <td>A lightweight chat model allowing accurate, and responsive output without requiring high-end hardware.</td>
      <td></td>
      <td>https://huggingface.co/search/full-text?q=stablelm-zephyr&type=model</td>
      <td>2929</td>
      <td>2023-12-30</td>
    </tr>
    <tr>
      <td>yarn-llama2</td>
      <td>An extension of Llama 2 that supports a context of up to 128k tokens.</td>
      <td></td>
      <td>https://huggingface.co/search/full-text?q=yarn-llama2&type=model</td>
      <td>2840</td>
      <td>2023-11-13</td>
    </tr>
    <tr>
      <td>deepseek-llm</td>
      <td>An advanced language model crafted with 2 trillion bilingual tokens.</td>
      <td>chinese</td>
      <td>https://huggingface.co/search/full-text?q=deepseek-llm&type=model</td>
      <td>2719</td>
      <td>2023-12-16</td>
    </tr>
    <tr>
      <td>magicoder</td>
      <td>ðŸŽ© Magicoder is a family of 7B parameter models trained on 75K synthetic instruction data using OSS-Instruct, a novel approach to enlightening LLMs with open-source code snippets.</td>
      <td>code</td>
      <td>https://huggingface.co/search/full-text?q=magicoder&type=model</td>
      <td>2643</td>
      <td>2023-12-09</td>
    </tr>
    <tr>
      <td>codebooga</td>
      <td>A high-performing code instruct model created by merging two existing code models.</td>
      <td>code</td>
      <td>https://huggingface.co/search/full-text?q=codebooga&type=model</td>
      <td>2188</td>
      <td>2023-11-13</td>
    </tr>
    <tr>
      <td>mistrallite</td>
      <td>MistralLite is a fine-tuned model based on Mistral with enhanced capabilities of processing long contexts.</td>
      <td></td>
      <td>https://huggingface.co/search/full-text?q=mistrallite&type=model</td>
      <td>2023</td>
      <td>2023-11-13</td>
    </tr>
    <tr>
      <td>nous-hermes2</td>
      <td>The powerful family of models by Nous Research that excels at scientific discussion and coding tasks.</td>
      <td>code</td>
      <td>https://huggingface.co/search/full-text?q=nous-hermes2&type=model</td>
      <td>1989</td>
      <td>2024-01-02</td>
    </tr>
    <tr>
      <td>goliath</td>
      <td>A language model created by combining two fine-tuned Llama 2 70B models into one.</td>
      <td></td>
      <td>https://huggingface.co/search/full-text?q=goliath&type=model</td>
      <td>1743</td>
      <td>2023-11-18</td>
    </tr>
    <tr>
      <td>nexusraven</td>
      <td>Nexus Raven is a 13B instruction tuned model for function calling tasks.</td>
      <td></td>
      <td>https://huggingface.co/search/full-text?q=nexusraven&type=model</td>
      <td>1616</td>
      <td>2024-01-09</td>
    </tr>
    <tr>
      <td>wizardlm</td>
      <td>General use 70 billion parameter model based on Llama 2.</td>
      <td></td>
      <td>https://huggingface.co/search/full-text?q=wizardlm&type=model</td>
      <td>1612</td>
      <td>2023-11-13</td>
    </tr>
    <tr>
      <td>alfred</td>
      <td>A robust conversational model designed to be used for both chat and instruct use cases.</td>
      <td></td>
      <td>https://huggingface.co/search/full-text?q=alfred&type=model</td>
      <td>1327</td>
      <td>2023-11-25</td>
    </tr>
    <tr>
      <td>notux</td>
      <td>A top-performing mixture of experts model, fine-tuned with high-quality data.</td>
      <td></td>
      <td>https://huggingface.co/search/full-text?q=notux&type=model</td>
      <td>1219</td>
      <td>2024-01-02</td>
    </tr>
    <tr>
      <td>xwinlm</td>
      <td>Conversational model based on Llama 2 that performs competitively on various benchmarks.</td>
      <td></td>
      <td>https://huggingface.co/search/full-text?q=xwinlm&type=model</td>
      <td>1169</td>
      <td>2023-11-13</td>
    </tr>
    <tr>
      <td>llama-pro</td>
      <td>An expansion of Llama 2 that specializes in integrating both general language understanding and domain-specific knowledge, particularly in programming and mathematics.</td>
      <td>code / math</td>
      <td>https://huggingface.co/search/full-text?q=llama-pro&type=model</td>
      <td>948</td>
      <td>2024-01-09</td>
    </tr>
    <tr>
      <td>notus</td>
      <td>A 7B chat model fine-tuned with high-quality data and based on Zephyr.</td>
      <td></td>
      <td>https://huggingface.co/search/full-text?q=notus&type=model</td>
      <td>674</td>
      <td>2024-01-02</td>
    </tr>
    <tr>
      <td>megadolphin</td>
      <td>MegaDolphin-2.2-120b is a transformation of Dolphin-2.2-70b created by interleaving the model with itself.</td>
      <td></td>
      <td>https://huggingface.co/search/full-text?q=megadolphin&type=model</td>
      <td>449</td>
      <td>2024-01-11</td>
    </tr>
  </tbody>
</table>